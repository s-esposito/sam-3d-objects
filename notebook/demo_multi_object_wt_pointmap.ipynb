{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Copyright (c) Meta Platforms, Inc. and affiliates."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Imports and Model Loading"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import uuid\n",
                "import imageio\n",
                "import numpy as np\n",
                "import torch\n",
                "from IPython.display import Image as ImageDisplay\n",
                "from pytorch3d.transforms import Transform3d\n",
                "\n",
                "from inference import Inference, ready_gaussian_for_video_rendering, load_image, load_masks, display_image, make_scene, render_video, interactive_visualizer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "PATH = os.getcwd()\n",
                "TAG = \"hf\"\n",
                "config_path = f\"{PATH}/../checkpoints/{TAG}/pipeline.yaml\"\n",
                "inference = Inference(config_path, compile=False)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load input image to lift to 3D (multiple objects)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "IMAGE_PATH = f\"{PATH}/images/shutterstock_stylish_kidsroom_1640806567/image.png\"\n",
                "IMAGE_NAME = os.path.basename(os.path.dirname(IMAGE_PATH))\n",
                "\n",
                "image = load_image(IMAGE_PATH)\n",
                "masks = load_masks(os.path.dirname(IMAGE_PATH), extension=\".png\")\n",
                "display_image(image, masks)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run MoGe depth model to get pointmap\n",
                "# Access the depth model from the inference pipeline\n",
                "depth_model = inference._pipeline.depth_model\n",
                "\n",
                "# Prepare image for depth inference\n",
                "from sam3d_objects.data.dataset.tdfy.img_and_mask_transforms import get_mask\n",
                "\n",
                "loaded_image = inference._pipeline.image_to_float(image)\n",
                "loaded_image = torch.from_numpy(loaded_image)\n",
                "loaded_mask = loaded_image[..., -1]\n",
                "loaded_image_rgb = loaded_image.permute(2, 0, 1).contiguous()[:3]\n",
                "\n",
                "# Run depth inference\n",
                "with torch.no_grad():\n",
                "    with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
                "        depth_output = depth_model(loaded_image_rgb)\n",
                "\n",
                "# Extract pointmap and convert to PyTorch3D camera convention\n",
                "pointmap = depth_output[\"pointmaps\"]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Camera convention transformation (R3 -> PyTorch3D)\n",
                "from pytorch3d.renderer import look_at_view_transform\n",
                "\n",
                "r3_to_p3d_R, r3_to_p3d_T = look_at_view_transform(\n",
                "    eye=np.array([[0, 0, -1]]),\n",
                "    at=np.array([[0, 0, 0]]),\n",
                "    up=np.array([[0, -1, 0]]),\n",
                "    device=pointmap.device,\n",
                ")\n",
                "\n",
                "camera_convention_transform = Transform3d(device=pointmap.device).rotate(r3_to_p3d_R)\n",
                "pointmap = camera_convention_transform.transform_points(pointmap)\n",
                "\n",
                "print(f\"Pointmap shape: {pointmap.shape}\")\n",
                "print(f\"Pointmap min: {pointmap.min()}, max: {pointmap.max()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize pointmap\n",
                "import matplotlib.pyplot as plt\n",
                "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
                "\n",
                "# Assuming pointmap is a tensor of shape (H, W, 3)\n",
                "pointmap_np = pointmap.cpu().numpy()\n",
                "\n",
                "# Map position to RGB colors for visualization\n",
                "normed_x = (pointmap_np[..., 0] - pointmap_np[..., 0].min()) / (pointmap_np[..., 0].max() - pointmap_np[..., 0].min())\n",
                "normed_y = (pointmap_np[..., 1] - pointmap_np[..., 1].min()) / (pointmap_np[..., 1].max() - pointmap_np[..., 1].min())\n",
                "normed_z = (pointmap_np[..., 2] - pointmap_np[..., 2].min()) / (pointmap_np[..., 2].max() - pointmap_np[..., 2].min())\n",
                "color_map = np.stack([normed_x, normed_y, normed_z], axis=-1)\n",
                "\n",
                "# Create figure with subplots\n",
                "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
                "\n",
                "# Visualize color encoding of pointmap\n",
                "ax1.imshow(color_map)\n",
                "ax1.set_title('Pointmap Color Visualization', fontsize=14)\n",
                "ax1.axis('off')\n",
                "\n",
                "# Visualize the depth (z-coordinate) with matching colorbar height\n",
                "im = ax2.imshow(pointmap_np[..., 2], cmap='plasma')\n",
                "ax2.set_title('Pointmap Depth Visualization', fontsize=14)\n",
                "ax2.axis('off')\n",
                "\n",
                "# Create colorbar with same height as the image\n",
                "divider = make_axes_locatable(ax2)\n",
                "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
                "plt.colorbar(im, cax=cax, label='Depth (Z-coordinate)')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize 3D point cloud using Plotly (works in remote Jupyter notebooks)\n",
                "import plotly.graph_objects as go\n",
                "\n",
                "# Reshape pointmap to (N, 3) where N = H * W\n",
                "points_3d = pointmap_np.reshape(-1, 3)\n",
                "\n",
                "# Get colors from the original image (RGB values in range [0, 255])\n",
                "image_rgb = loaded_image_rgb.cpu().numpy().transpose(1, 2, 0)  # Convert to (H, W, 3)\n",
                "colors_rgb = (image_rgb * 255).astype(np.uint8)  # Convert to 0-255 range\n",
                "colors_flat = colors_rgb.reshape(-1, 3)\n",
                "\n",
                "# Optional: Downsample for better performance (every Nth point)\n",
                "max_nr_points = 100000  # Adjust this based on performance needs\n",
                "if points_3d.shape[0] > max_nr_points:\n",
                "    downsample_factor = points_3d.shape[0] // max_nr_points\n",
                "else:\n",
                "    downsample_factor = 100  # Adjust this to control point cloud density\n",
                "points_downsampled = points_3d[::downsample_factor]\n",
                "colors_downsampled = colors_flat[::downsample_factor]\n",
                "\n",
                "# Filter outliers\n",
                "valid_mask = np.linalg.norm(points_downsampled, axis=1) < 3.0\n",
                "points_filtered = points_downsampled[valid_mask]\n",
                "colors_filtered = colors_downsampled[valid_mask]\n",
                "\n",
                "print(f\"Original points: {len(points_3d):,}\")\n",
                "print(f\"Downsampled points: {len(points_downsampled):,}\")\n",
                "print(f\"Filtered points: {len(points_filtered):,}\")\n",
                "\n",
                "# Create RGB color strings for Plotly\n",
                "rgb_colors = [f'rgb({r},{g},{b})' for r, g, b in colors_filtered]\n",
                "\n",
                "# Create 3D scatter plot\n",
                "fig = go.Figure(data=[go.Scatter3d(\n",
                "    x=points_filtered[:, 0],\n",
                "    y=points_filtered[:, 1],\n",
                "    z=points_filtered[:, 2],\n",
                "    mode='markers',\n",
                "    marker=dict(\n",
                "        size=2,\n",
                "        color=rgb_colors,\n",
                "        opacity=0.8\n",
                "    ),\n",
                "    text=[f'({x:.2f}, {y:.2f}, {z:.2f})' for x, y, z in points_filtered],\n",
                "    hoverinfo='text'\n",
                ")])\n",
                "\n",
                "# Update layout for better visualization\n",
                "fig.update_layout(\n",
                "    title='3D Point Cloud from MoGe Depth Estimation',\n",
                "    scene=dict(\n",
                "        xaxis_title='X',\n",
                "        yaxis_title='Y',\n",
                "        zaxis_title='Z',\n",
                "        aspectmode='data',\n",
                "        camera=dict(\n",
                "            eye=dict(x=1.5, y=1.5, z=1.5)\n",
                "        )\n",
                "    ),\n",
                "    width=1000,\n",
                "    height=800,\n",
                "    margin=dict(l=0, r=0, t=40, b=0)\n",
                ")\n",
                "\n",
                "# Show interactive plot in notebook\n",
                "fig.show()\n",
                "\n",
                "print(\"\\nInteractive controls:\")\n",
                "print(\"- Click and drag to rotate\")\n",
                "print(\"- Scroll to zoom\")\n",
                "print(\"- Double-click to reset view\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Generate Gaussian Splats"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Pass the pointmap to inference for all masks\n",
                "# The pointmap is shared across all objects in the same scene\n",
                "\n",
                "outputs = [inference(image, mask, seed=42, pointmap=pointmap) for mask in masks]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Visualize Gaussian Splat of the Scene\n",
                "### a. Animated Gif"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "scene_gs = make_scene(*outputs)\n",
                "scene_gs = ready_gaussian_for_video_rendering(scene_gs)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# export gaussian splatting (as point cloud)\n",
                "scene_gs.save_ply(f\"{PATH}/gaussians/multi_wt_moge/{IMAGE_NAME}.ply\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# render video\n",
                "video = render_video(\n",
                "    scene_gs,\n",
                "    r=1,\n",
                "    fov=60,\n",
                "    resolution=512,\n",
                ")[\"color\"]\n",
                "\n",
                "# save video as gif\n",
                "imageio.mimsave(\n",
                "    os.path.join(f\"{PATH}/gaussians/multi_wt_moge/{IMAGE_NAME}.gif\"),\n",
                "    video,\n",
                "    format=\"GIF\",\n",
                "    duration=1000 / 30,  # default assuming 30fps from the input MP4\n",
                "    loop=0,  # 0 means loop indefinitely\n",
                ")\n",
                "\n",
                "# notebook display\n",
                "ImageDisplay(url=f\"gaussians/multi_wt_moge/{IMAGE_NAME}.gif?cache_invalidator={uuid.uuid4()}\",)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "sam3d-objects",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
